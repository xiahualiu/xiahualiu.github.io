<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.61.0" />
  
  
  
  <title>
    
    Matrix Calculus | Xiahua
    
  </title>
  <link rel="canonical" href="https://xiahualiu.github.io/posts/2020-01-30-matrix-multiplication/">
  
  
  
  
  
  
  
  
  <link rel="stylesheet" href="https://xiahualiu.github.io/css/base.min.a325db5a8beaa47e0541e6117eb67c3bc54cf8945100141e8a9defb2533f7344.css" integrity="sha256-oyXbWovqpH4FQeYRfrZ8O8VM&#43;JRRABQeip3vslM/c0Q=" crossorigin="anonymous">
  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <a class="Banner-link u-clickable" href="https://xiahualiu.github.io">Xiahua</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://github.com/xiahualiu">About</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://xiahualiu.github.io/index.xml">RSS</a>
      </li>
      
    </ul>
  </div>
</nav>

  <main>
    <div class="u-wrapper">
      <div class="u-padding">
        

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://xiahualiu.github.io/posts/2020-01-30-matrix-multiplication/" rel="bookmark">Matrix Calculus</a>
  </h2>
  
  <time datetime="2020-01-30T20:17:54-05:00">
    30 January, 2020
  </time>
  
</header>

  <p>Matrix calculus is very important in machine learning and control theories. However there are all kinds of rules making it hard to remember and use. In this post, I summarize some basic rules and an important proof in machine learning theory, which includes the famous &quot;Trace Trick&quot;! This trick is super useful and super hard as well! Haha, this is the reason why we always explore new things, because it is hard!</p>

<h2 id="differentiation">Differentiation</h2>

<p>For the rules of matrix differentiation, all we need to remember are:</p>

<ul>
<li>Gradient</li>
<li>The Jacobian matrix</li>
<li>The Hessian matrix</li>
</ul>

<p>* All vectors in this post refer to column vectors with dimension <span  class="math">\(N\)</span>.</p>

<h3 id="getting-started">Getting started</h3>

<p>To start with the concept, we differentiate a matrix with respect to a scalar variable, the result is pretty intuitive and natural:</p>

<p><span  class="math">\[\dot{A}=\begin{bmatrix}
\dot{A}_{11} & \cdots & \dot{A}_{1n} \\
\vdots & \ddots & \vdots \\
\dot{A}_{n1} & \cdots & \dot{A}_{nn} 
\end{bmatrix}\]</span></p>

<h3 id="gradient">Gradient</h3>

<p>The gradient of a scalar function can be visualized as the changes in different directions.</p>

<p><span  class="math">\[\nabla f(\textbf{x})=\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}\]</span></p>

<p>It is the most simplifed case, remember it returns a vector that's in the same shape as the input vector variable.</p>

<h3 id="jacobian-matrix">Jacobian Matrix</h3>

<p>Jacobian matrix is the first-order derivative of a <strong>vector function</strong>, a vector function is a function that <strong>receives a vector value and returns a vector value</strong>.</p>

<p>Jacobian matrix is obtained from <span  class="math">\((\nabla \textbf{f})^{\top}\)</span>.</p>

<p><span  class="math">\[(\nabla \textbf{f})^{\top}=[\frac{\partial \textbf{f}}{\partial x_1} \cdots \frac{\partial \textbf{f}}{\partial x_n}]=\begin{bmatrix}
\frac{\partial f_1}{\partial x_1}  & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}\]</span></p>

<p>We did gradient operation at first, transposed the result, then applied the simple first derivative expansion as discussed in the warm up section.</p>

<h2 id="hessian-matrix">Hessian Matrix</h2>

<p>Hessian matrix is the second-order partial derivatives of a <strong>scalar-valued</strong> function. A scalar-valued function is a function that <strong>receives a scalar or vector and returns a scalar</strong>. Here we assume it the function receives a vector.</p>

<p>Since it is a scalar function, first we find the gradient:</p>

<p><span  class="math">\[\frac{\partial y}{\partial \textbf{x}}=\begin{bmatrix} \frac{\partial y}{\partial x_1} \\
\vdots \\
\frac{\partial y}{\partial x_n} \\
\end{bmatrix}=\nabla \textbf{f}\]</span></p>

<p>And we need to do this one more time.A sharp eye reader may notice the second derivative is the Jacobian matrix of the gradient, which is:</p>

<p><span  class="math">\[\textbf{J}(\nabla \textbf{f})\]</span></p>

<p>However, <strong>Hessian matrix is defined with an extra tranpose operation</strong>, so it is actually:</p>

<p><span  class="math">\[\textbf{J}(\nabla \textbf{f})^{\top}\]</span></p>

<p><span  class="math">\[\mathbf{H}=\left[\begin{array}{cccc}
{\frac{\partial^{2} f}{\partial x_{1}^{2}}} & {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\
{\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} & {\frac{\partial^{2} f}{\partial x_{2}^{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} \\
{\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} & {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{n}^{2}}}
\end{array}\right]\]</span></p>

<p>Till now I hope you can understand what Jacobian matrix and Hessian matrix are, and be able to write the steps how we get Jacobian and Hessian matrix.</p>

<h2 id="integral">Integral</h2>

<p>Calculating the integral of the whole matrix is as simple as the warm up part.</p>

<p><span  class="math">\[\int A(t) {\rm d}t=\begin{bmatrix}
\int A_{11}{\rm d}t & \cdots & \int A_{1n}(t){\rm d}t \\
\vdots & \ddots & \vdots \\
\int A_{n1}(t){\rm d}t & \cdots & \int A_{nn}(t){\rm d}t
\end{bmatrix}\]</span></p>

<h2 id="trace-trick">Trace trick</h2>

<p>In this section, we will teach you what trace trick is. As usual, we start with a simple case:</p>

<h3 id="derivative-of-a-quadratic-form">Derivative of a quadratic form</h3>

<p><strong>If you have no idea what a quadratic form is, I suggest you stop reading downwards</strong>, because you have to have a good understanding of it, otherwise you cannot realize why we want to find its derivative.</p>

<p>I also suggest a wonderful book for that foundation, <em>Linear Algebra and its Applications</em>, this is a must-read for all new learners.</p>

<p>So here we begin!</p>

<p>Define a function:</p>

<p><span  class="math">\[f(\textbf{x})=\textbf{x}^{\top}\textbf{Ax}\]</span></p>

<p>This is a scalar function, since <span  class="math">\(\textbf{x}\)</span> is <span  class="math">\(m\times 1\)</span>. So the size of the output is:</p>

<p><span  class="math">\[(1\times m)\times(m\times m)\times(m\times 1)=(1\times 1)\]</span></p>

<p>And we need to find its first derivative w.r.t <span  class="math">\(\textbf{x}\)</span>. How?</p>

<p>You may think, it is same as finding the gradient of the function, so just expand the equation and find the partial derivative.</p>

<p>Yes, it is indeed a reasonal idea, but expanding all the terms is way to tedious and humans are fallible. Is there any other way to obtain the result without messing the matrix?</p>

<p>Yes, there is. It is called <strong>&quot;Trace Trick&quot;</strong> and is widely used in finding the derivative of quadratic form function.</p>

<h3 id="trace">Trace</h3>

<p>From wikipedia:</p>

<blockquote>
<p>In linear algebra, the trace of a square matrix <span  class="math">\(A\)</span> is defined to be the sum of elements on the main diagonal of <span  class="math">\(A\)</span>. The trace of a matrix is the sum of its eigenvalues, and it is invariant with respect to a change of basis.</p>
</blockquote>

<h3 id="trace-laws">Trace laws</h3>

<p>Here are two most important laws:</p>

<h4 id="transpose-law">Transpose law</h4>

<p><span  class="math">\[\operatorname{tr}(\mathbf{A})=\operatorname{tr}\left(\mathbf{A}^{\top}\right)\]</span></p>

<h4 id="cyclic-permutation-law">Cyclic permutation law</h4>

<p><span  class="math">\[\operatorname{tr}(\mathbf{A B C D})=\operatorname{tr}(\mathbf{B C D A})=\operatorname{tr}(\mathbf{C D A B})=\operatorname{tr}(\mathbf{D A B C})\]</span></p>

<p>How do we apply the trace trick in our calculation? Well, from the definition of the trace, a trace of a scalar (which can be considered as a <span  class="math">\(1\times 1\)</span> matrix) is itself, so:</p>

<p><span  class="math">\[\operatorname{tr}(\textbf{x}^{\top}\textbf{Ax})=\textbf{x}^{\top}\textbf{Ax}\]</span></p>

<p>Now it is the trick time, I will write everything here, please read the following equation, and try to understand how I get to the final line:</p>

<p><span  class="math">\[\begin{aligned}
{\rm d}(\textbf{x}^{\top}\textbf{Ax}) &={\rm d}(\operatorname{tr}(\textbf{x}^{\top}\textbf{Ax})) \\
&=\operatorname{tr}({\rm d}(\textbf{Ax}\textbf{x}^{\top})) \\
&=\operatorname{tr}({\rm d}(\textbf{Ax})\textbf{x}^{\top}+{\bf Ax}{\rm d}(\textbf{x}^{\top})) \\
&=\operatorname{tr}({\rm d}(\textbf{Ax})\textbf{x}^{\top})+\operatorname{tr}({\bf Ax}{\rm d}(\textbf{x}^{\top})) \\
&=\operatorname{tr}(\textbf{A}({\rm d}\textbf{x})\textbf{x}^{\top})+\operatorname{tr}\left((({\rm d}\textbf{x})\textbf{x}^{\top} \textbf{A}^{\top})^{\top}\right) \\
&=\operatorname{tr}((\textbf{x}^{\top}\textbf{A}({\rm d}\textbf{x})) + \operatorname{tr}(\textbf{x}^{\top}\textbf{A}^{\top}({\rm d}\textbf{x})) \\
&=\operatorname{tr}(\textbf{x}^{\top}\textbf{A}){\rm d}\textbf{x} + \operatorname{tr}(\textbf{x}^{\top}\textbf{A}^{\top}){\rm d}\textbf{x} \\
&=\operatorname{tr}\left((\textbf{x}^{\top}\textbf{A}+\textbf{x}^{\top}\textbf{A}^{\top}){\rm d}\textbf{x}\right) \\
&=(\textbf{x}^{\top}\textbf{A}+\textbf{x}^{\top}\textbf{A}^{\top}){\rm d}\textbf{x}
\end{aligned}\]</span></p>

<p>I used cyclic permutation law at beginning and at the end twice, and used transpose law only once to transform <span  class="math">\({\rm d}\textbf{x}^{\top}\)</span>. I installed trace function on first line and uninstall the trace function on the last line, <span  class="math">\({\rm d}\textbf{x}=\lim(\textbf{x}_1-\textbf{x}_2)\in \Bbb{R}^{N}\)</span>, it can be proved that the expression on final line is still a scalar.</p>

<p>And we can get the result <span  class="math">\(\textbf{x}^{\top}\textbf{A}+\textbf{x}^{\top}\textbf{A}^{\top}\)</span> simply in several steps!</p>

<h4 id="another-important-law">Another Important Law</h4>

<p><span  class="math">\[\frac{\partial}{\partial A}\operatorname{tr}(AB)=B^{\top}\]</span></p>

<p>This law will be used once in the next section.</p>

<h2 id="multivariate-guassian-distribution-maximum-likelihood-estimate">Multivariate Guassian Distribution Maximum Likelihood Estimate</h2>

<p>In the end, I want to show you how to calculate the MLE for a multivariate guassian distribution model with all the stuff discussed in this post, I really hope you can enjoy it as much as I do!</p>

<h4 id="problem">Problem:</h4>

<p>Given <span  class="math">\(N\)</span> samples <span  class="math">\(\{X_1,X_2,X_3,...,X_i\}\)</span> are drawn from a p-variate multivariate guassian distribution model i.i.d:</p>

<p><span  class="math">\[p(x | \mu, \Sigma)=\frac{1}{(2 \pi)^{p / 2}|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\}\]</span></p>

<p>Find the MLE for <span  class="math">\(\mu\ \Sigma\)</span>.</p>

<h4 id="solution">Solution:</h4>

<p>Define the likelihood function <span  class="math">\(L\)</span>:</p>

<p><span  class="math">\[f(\mu, \Sigma)=\frac{1}{(2\pi)^{\frac{pN}{2}}|\Sigma|^{\frac{N}{2}}}\exp{\left\{\sum_{i=1}^{N}-\frac{1}{2}(X_i-\mu)^{\top}\Sigma^{-1}(X_i-\mu)\right\}}\]</span></p>

<p>In order to simplify the problem, plus due to the fact that the multivariate guassian distribution belongs to the exponential distribution family. Instead straight differentiating the likelihood function, it is better to calculate the logarithmic differentiation of the function.</p>

<p><span  class="math">\[\begin{aligned}
& L(\mu, \Sigma)=\ln f(\mu, \Sigma)= \\
& \sum_{i=1}^{N}-\frac{1}{2}(X_i-\mu)^{\top}\Sigma^{-1}(X_i-\mu)-\frac{pN}{2}\ln 2\pi - \frac{N}{2}\ln|\Sigma|\end{aligned}\]</span></p>

<h5 id="find-the-mle-for-the-the-mean">Find the MLE for the the mean</h5>

<p>Find <span  class="math">\(\mu\)</span> first, then <span  class="math">\(\Sigma\)</span>, calculate the first-order partial derivative of <span  class="math">\(L(\mu, \Sigma)\)</span> w.r.t <span  class="math">\(\mu\)</span>.</p>

<p><span  class="math">\[\frac{\partial L(\mu, \Sigma)}{\partial \mu} = \sum_{i=1}^n - \partial \frac{1}{2}(X_i-\mu)^{\top}\Sigma^{-1}(X_i-\mu) /\partial \mu \]</span></p>

<p>And if we regard <span  class="math">\((X_i-\mu)\)</span> as <span  class="math">\(\textbf{x}\)</span> in the former section, the partial derivative on the right side is simply:</p>

<p><span  class="math">\[\frac{1}{2}\sum_{i=1}^{N} (X_i-\mu)^{\top}\Sigma^{-1}+ (X_i-\mu)^{\top}(\Sigma^{-1})^T\]</span></p>

<p>Since <span  class="math">\(\Sigma\)</span> is the covariance matrix, it is symmetric:</p>

<p><span  class="math">\[\begin{aligned}
\Sigma^{-1}\Sigma&=(\Sigma\Sigma^{-1})^{\top} \\
\Sigma^{-1}\Sigma&=(\Sigma^{-1})^{\top}\Sigma^{\top} \\
\Sigma^{-1}&=(\Sigma^{-1})^{\top}
\end{aligned}\]</span></p>

<p>The partial derivative w.r.t <span  class="math">\(\mu\)</span> in the conclusion:</p>

<p><span  class="math">\[\frac{\partial L(\mu, \Sigma)}{\partial \mu} =\sum_{i=1}^n(X_i-\mu)^{\top}\Sigma^{-1}\]</span></p>

<p>Find its critical point, i.e. the mean of all samples:</p>

<p><span  class="math">\[\hat{\mu}=\frac{1}{N}\sum_{i=1}^{N}X_i\]</span></p>

<h5 id="find-the-mle-for-the-covariance-matrix">Find the MLE for the covariance matrix</h5>

<p>Since we get <span  class="math">\(\hat{\mu}\)</span>, and apparently the <span  class="math">\(\mu\)</span> estimate does not rely on <span  class="math">\(\Sigma\)</span>, these two parameters are independent. We can safely substitute all <span  class="math">\(\mu\)</span> with <span  class="math">\(\hat{\mu}\)</span>.</p>

<p><span  class="math">\[\begin{aligned}
\frac{\partial L(\hat{\mu}, \Sigma)}{\partial \Sigma^{-1}}&=\partial\left(-\frac{N}{2}\ln|\Sigma|\right)/\partial\Sigma^{-1}-\frac{1}{2}\sum_{i=1}^{N}(X_i-\hat{\mu})(X_i-\hat{\mu})^{\top} \\
&=\partial (\frac{N}{2}\ln|\Sigma^{-1}|)/\partial \Sigma^{-1} -\frac{1}{2}\sum_{i=1}^{N}(X_i-\hat{\mu})(X_i-\hat{\mu})^{\top} \\
&=\frac{N}{2\Sigma^{-1}}-\frac{1}{2}\sum_{i=1}^{N}(X_i-\hat{\mu})(X_i-\hat{\mu})^{\top} \\
&=\frac{N}{2}\Sigma-\frac{1}{2}\sum_{i=1}^{N}(X_i-\hat{\mu})(X_i-\hat{\mu})^{\top}
\end{aligned} \]</span></p>

<p>The last term on the first line was obtained by first using the cyclic permutaion then using the extra law in the former section.</p>

<p><span  class="math">\[\frac{1}{2} \sum_{n}\left(x_{n}-\mu\right)^{\top} \Sigma^{-1}\left(x_{n}-\mu\right)=\frac{1}{2} \sum_{i=1}^{N} \operatorname{tr}\left[\left(X_{i}-\hat{\mu}\right)\left(X_{i}-\hat{\mu}\right)^{\top} \Sigma^{-1}\right]\]</span></p>

<p>Then the extra law was used.</p>

<p><span  class="math">\[\begin{aligned}
& \frac{\partial}{\partial\Sigma^{-1}}(\frac{1}{2} \sum_{i=1}^{N} \operatorname{tr}\left[\left(X_{i}-\hat{\mu}\right)\left(X_{i}-\hat{\mu}\right)^{\top} \Sigma^{-1}\right]) \\
= & \frac{1}{2}\sum_{i=1}^{N}(X_i-\hat{\mu})(X_i-\hat{\mu})^{\top}
\end{aligned}\]</span></p>
  







  



</article>


      </div>
    </div>
  </main>
  
<footer class="Footer">
  <div class="u-wrapper">
    <div class="u-padding">
      Enjoy yourself! All contents are under CC0-1.0 license!
    </div>
  </div>
</footer>


</body>
</html>
