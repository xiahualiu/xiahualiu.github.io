<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.61.0" />
  
  
  
  <title>
    
    Matrix Calculus | Xiahua
    
  </title>
  <link rel="canonical" href="https://xiahualiu.github.io/posts/2020-01-30-matrix-multiplication/">
  
  
  
  
  
  
  
  
  <link rel="stylesheet" href="https://xiahualiu.github.io/css/base.min.a325db5a8beaa47e0541e6117eb67c3bc54cf8945100141e8a9defb2533f7344.css" integrity="sha256-oyXbWovqpH4FQeYRfrZ8O8VM&#43;JRRABQeip3vslM/c0Q=" crossorigin="anonymous">
  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <a class="Banner-link u-clickable" href="https://xiahualiu.github.io">Xiahua</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://github.com/xiahualiu">About</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://xiahualiu.github.io/index.xml">RSS</a>
      </li>
      
    </ul>
  </div>
</nav>

  <main>
    <div class="u-wrapper">
      <div class="u-padding">
        

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://xiahualiu.github.io/posts/2020-01-30-matrix-multiplication/" rel="bookmark">Matrix Calculus</a>
  </h2>
  
  <time datetime="2020-01-30T20:17:54-05:00">
    30 January, 2020
  </time>
  
</header>

  <p>Matrix calculus is very important in machine learning and control theories. However there are all kinds of rules making it hard to remember and use. In this post, I summarize some basic rules and an important proof in machine learning theory, which includes the famous &quot;Trace Trick&quot;! This trick is super useful and super hard as well! Haha, this is the reason why we always explore new things, because it is hard!</p>

<h2 id="differentiation">Differentiation</h2>

<p>For the rules of matrix differentiation, all we need to remember are:</p>

<ul>
<li>Gradient</li>
<li>the Jacobian matrix</li>
<li>the Hessian matrix</li>
</ul>

<p>* All vectors in this post refer to column vectors with dimension <span  class="math">\(N\)</span>.</p>

<h3 id="getting-started">Getting started</h3>

<p>So for a warm up part, we differentiate a matrix with respect to a scalar variable, the result is pretty intuitive and natural:</p>

<p><span  class="math">\[\dot{A}=\begin{bmatrix}
\dot{A}_{11} & \cdots & \dot{A}_{1n} \\
\vdots & \ddots & \vdots \\
\dot{A}_{n1} & \cdots & \dot{A}_{nn} 
\end{bmatrix}\]</span></p>

<h3 id="gradient">Gradient</h3>

<p>The gradient of a scalar function can be visualized as the changes in different directions.</p>

<p><span  class="math">\[\nabla f(\textbf{x})=\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}\]</span></p>

<p>It is the most simplifed case, remember it returns a vector that's in the same shape as the vector variable.</p>

<h3 id="jacobian-matrix">Jacobian Matrix</h3>

<p>Jacobian matrix is the first-order derivative of a <strong>vector function</strong>, a vector function is a function that <strong>receives a vector value and returns a vector value</strong>.</p>

<p>Jacobian matrix is obtained from <span  class="math">\((\nabla \textbf{f})^{\top}\)</span>, they are equivalent.</p>

<p><span  class="math">\[\frac{\partial \textbf{y}}{\partial \textbf{x}}=[\frac{\partial \textbf{f}}{\partial x_1} \cdots \frac{\partial \textbf{f}}{\partial x_n}]=\begin{bmatrix}
\frac{\partial f_1}{\partial x_1}  & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}\]</span></p>

<p>We did gradient operation at first, transposed the result, then applied the simple first derivative expansion as discussed in the warm up section.</p>

<h2 id="hessian-matrix">Hessian Matrix</h2>

<p>Hessian matrix is the second-order partial derivatives of a <strong>scalar-valued</strong> function. A scalar-valued function is a function that <strong>receives a scalar or vector and returns a scalar</strong>. Here we assume it the function receives a vector.</p>

<p>Since it is a scalar function, first we find the gradient:</p>

<p><span  class="math">\[\frac{\partial y}{\partial \textbf{x}}=\begin{bmatrix} \frac{\partial y}{\partial x_1} \\
\vdots \\
\frac{\partial y}{\partial x_n} \\
\end{bmatrix}=\nabla \textbf{f}\]</span></p>

<p>And we need to do this one more time.A sharp eye reader may notice the second derivative is the Jacobian matrix of the gradient, which is:</p>

<p><span  class="math">\(\textbf{J}(\nabla \textbf{f})\)</span>:</p>

<p>However, <strong>Hessian matrix is defined with an extra tranpose operation</strong>, so it is actually:</p>

<p><span  class="math">\(\textbf{J}(\nabla \textbf{f})^{\top}\)</span>:</p>

<p><span  class="math">\[\mathbf{H}=\left[\begin{array}{cccc}
{\frac{\partial^{2} f}{\partial x_{1}^{2}}} & {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\
{\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} & {\frac{\partial^{2} f}{\partial x_{2}^{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} \\
{\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} & {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{n}^{2}}}
\end{array}\right]\]</span></p>

<p>So till now I hope you can write the steps how we get Jacobian and Hessian matrix.</p>

<h2 id="integral">Integral</h2>

<p>And calculating the integral of the whole matrix is simple as the warm up part.</p>

<p><span  class="math">\[\int A(t) {\rm d}t=\begin{bmatrix}
\int A_{11}{\rm d}t & \cdots & \int A_{1n}(t){\rm d}t \\
\vdots & \ddots & \vdots \\
\int A_{n1}(t){\rm d}t & \cdots & \int A_{nn}(t){\rm d}t
\end{bmatrix}\]</span></p>

<h2 id="trace-trick">Trace trick</h2>

<p>In this section, we will teach you how to understand trace trick and as usual, we start with a simple case:</p>

<h3 id="derivative-of-a-quadratic-form">Derivative of a quadratic form</h3>

<p><strong>If you have no idea what a quadratic form is, I suggest you stop reading downwards</strong>, because you have to have a good understanding of it, otherwise you will not realize the reason and the true importance of finding its derivative.</p>

<p>I also suggest a wonderful book for that foundation, <em>Linear Algebra and its Applications</em>, this is a must-read book for new learners.</p>

<p>So here we begin!</p>

<p>Define a function:</p>

<p><span  class="math">\[f(\textbf{x})=\textbf{x}^{\top}\textbf{Ax}\]</span></p>

<p>This is a scalar function, since <span  class="math">\(\textbf{x}\)</span> is <span  class="math">\(m\times 1\)</span>. So the size of the output is:</p>

<p><span  class="math">\[(1\times m)\times(m\times m)\times(m\times 1)=(1\times 1)\]</span></p>

<p>And we need to find its first derivative w.r.t <span  class="math">\(\textbf{x}\)</span>. How?</p>

<p>You may think, it is same as finding the gradient of the function, so just expand the equation and find partial derivative.</p>

<p>Yes, it is indeed a good idea, but expanding all the terms is way to tedious and humans are fallible. Is there any other way around?</p>

<p>Yes, there is. It is called <strong>Trace Trick</strong> and is widely used in finding the derivative of quadratic form function.</p>

<h3 id="trace">Trace</h3>

<p>From wikipedia:</p>

<blockquote>
<p>In linear algebra, the trace of a square matrix <span  class="math">\(A\)</span> is defined to be the sum of elements on the main diagonal of <span  class="math">\(A\)</span>. The trace of a matrix is the sum of its eigenvalues, and it is invariant with respect to a change of basis.</p>
</blockquote>

<h3 id="trace-laws">Trace laws</h3>

<p>Here are two most important laws:</p>

<h4 id="transpose-law">Transpose law</h4>

<p><span  class="math">\[\operatorname{tr}(\mathbf{A})=\operatorname{tr}\left(\mathbf{A}^{\top}\right)\]</span></p>

<h4 id="cyclic-permutation-law">Cyclic permutation law</h4>

<p><span  class="math">\[\operatorname{tr}(\mathbf{A B C D})=\operatorname{tr}(\mathbf{B C D A})=\operatorname{tr}(\mathbf{C D A B})=\operatorname{tr}(\mathbf{D A B C})\]</span></p>

<p>How do we apply the trace trick in our calculation? Well, from the definition of the trace, a trace of a scalar (which can be considered as a <span  class="math">\(1\times 1\)</span> matrix) is itself, so:</p>

<p><span  class="math">\[\operatorname{tr}(\textbf{x}^{\top}\textbf{Ax})=\textbf{x}^{\top}\textbf{Ax}\]</span></p>

<p>Now it is the trick time:</p>

<p><span  class="math">\[\begin{aligned}
{\rm d}(\textbf{x}^{\top}\textbf{Ax}) &={\rm d}(\operatorname{tr}(\textbf{x}^{\top}\textbf{Ax})) \\
&=\operatorname{tr}({\rm d}(\textbf{Ax}\textbf{x}^{\top})) \\
&=\operatorname{tr}({\rm d}(\textbf{Ax})\textbf{x}^{\top}+{\bf Ax}{\rm d}(\textbf{x}^{\top})) \\
&=\operatorname{tr}({\rm d}(\textbf{Ax})\textbf{x}^{\top})+\operatorname{tr}({\bf Ax}{\rm d}(\textbf{x}^{\top})) \\
&=\operatorname{tr}(\textbf{A}({\rm d}\textbf{x})\textbf{x}^{\top})+\operatorname{tr}\left((({\rm d}\textbf{x})\textbf{x}^{\top} \textbf{A}^{\top})^{\top}\right) \\
&=\operatorname{tr}((\textbf{x}^{\top}\textbf{A}({\rm d}\textbf{x})) + \operatorname{tr}(\textbf{x}^{\top}\textbf{A}^{\top}({\rm d}\textbf{x})) \\
&=\operatorname{tr}(\textbf{x}^{\top}\textbf{A}){\rm d}\textbf{x} + \operatorname{tr}(\textbf{x}^{\top}\textbf{A}^{\top}){\rm d}\textbf{x} \\
&=\operatorname{tr}\left((\textbf{x}^{\top}\textbf{A}+\textbf{x}^{\top}\textbf{A}^{\top}){\rm d}\textbf{x}\right) \\
&=(\textbf{x}^{\top}\textbf{A}+\textbf{x}^{\top}\textbf{A}^{\top}){\rm d}\textbf{x}
\end{aligned}\]</span></p>

<p>We used cyclic permutation law at beginning and at the end twice, and used transpose law only once to transform <span  class="math">\({\rm d}\textbf{x}^{\top}\)</span></p>

<p>And we can get the result <span  class="math">\(\textbf{x}^{\top}\textbf{A}+\textbf{x}^{\top}\textbf{A}^{\top}\)</span> simply in several steps!</p>

<h2 id="differetiate-an-inverse">Differetiate an inverse</h2>

<p><span  class="math">\[\frac{\rm d}{{\rm d}t}(A^{-1})=-A^{-1}\dot{A}A^{-1}\]</span></p>
  







  



</article>


      </div>
    </div>
  </main>
  
<footer class="Footer">
  <div class="u-wrapper">
    <div class="u-padding">
      Enjoy yourself! All contents are under CC0-1.0 license!
    </div>
  </div>
</footer>


</body>
</html>
